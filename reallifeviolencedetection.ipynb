{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T02:50:55.526587Z","iopub.status.busy":"2024-02-24T02:50:55.525718Z","iopub.status.idle":"2024-02-24T02:50:59.448056Z","shell.execute_reply":"2024-02-24T02:50:59.447232Z","shell.execute_reply.started":"2024-02-24T02:50:55.526546Z"},"trusted":true},"outputs":[],"source":["# Importing necessary modules and libraries\n","from glob import glob \n","import shutil\n","import random\n","import sys\n","import os \n","import pandas as pd\n","import numpy as np\n","\n","\n","## Libraries for Data Augmentation\n","import pytorchvideo\n","from pytorchvideo.data import LabeledVideoDataset, Kinetics, make_clip_sampler, labeled_video_dataset\n","\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    Normalize,\n","    RandomShortSideScale,\n","    UniformTemporalSubsample,\n","    Permute\n",")\n","from torchvision.transforms import(\n","    Compose, \n","    Lambda,\n","    RandomCrop,\n","    RandomHorizontalFlip,\n","    Resize\n",")\n","\n","from torchvision.transforms._transforms_video import(\n","    CenterCropVideo, \n","    NormalizeVideo\n",")\n","\n","from torch.utils.data import DataLoader\n","\n","\n","# Import necessary libraries for real-life violence detection\n","\n","import torch.nn as nn\n","import torch\n","from torch.optim import lr_scheduler\n","from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n","from pytorch_lightning import LightningModule, seed_everything, Trainer\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from sklearn.metrics import classification_report\n","import torchmetrics\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T02:53:35.086286Z","iopub.status.busy":"2024-02-24T02:53:35.085383Z","iopub.status.idle":"2024-02-24T02:53:35.092273Z","shell.execute_reply":"2024-02-24T02:53:35.091300Z","shell.execute_reply.started":"2024-02-24T02:53:35.086252Z"},"trusted":true},"outputs":[],"source":["# Define a video transformation pipeline\n","\n","video_transform = Compose([\n","    ApplyTransformToKey(key = 'video',\n","    transform = Compose([\n","        UniformTemporalSubsample(20), \n","        Lambda(lambda x:x/255),\n","        Normalize((0.45,0.45,0.45), (0.225,0.225,0.225)),\n","        RandomShortSideScale(min_size = 248, max_size = 256),\n","        CenterCropVideo(224),\n","        RandomHorizontalFlip(p = .5)\n","    ]) ),\n","])"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T02:53:35.407180Z","iopub.status.busy":"2024-02-24T02:53:35.406260Z","iopub.status.idle":"2024-02-24T02:53:35.411099Z","shell.execute_reply":"2024-02-24T02:53:35.410121Z","shell.execute_reply.started":"2024-02-24T02:53:35.407136Z"},"trusted":true},"outputs":[],"source":["# path of the data\n","data_path = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset'"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T02:53:35.744856Z","iopub.status.busy":"2024-02-24T02:53:35.744475Z","iopub.status.idle":"2024-02-24T02:53:35.754326Z","shell.execute_reply":"2024-02-24T02:53:35.753273Z","shell.execute_reply.started":"2024-02-24T02:53:35.744827Z"},"trusted":true},"outputs":[],"source":["\n","def gen_the_local_dataset(path_to_dir, train_s=0.8, val=0.2, test=0.1):\n","    \"\"\"\n","    Generate the local dataset by splitting the files in the given directory into train, validation, and test sets.\n","    \n","    Args:\n","        path_to_dir (str): The path to the directory containing the files.\n","        train_s (float): The proportion of files to be included in the train set (default is 0.8).\n","        val (float): The proportion of files to be included in the validation set (default is 0.2).\n","        test (float): The proportion of files to be included in the test set (default is 0.1).\n","    \n","    Returns:\n","        tuple: A tuple containing the paths to the train, validation, and test sets.\n","    \"\"\"\n","    \n","    # Create a dictionary to store the proportions for each set\n","    koef = {0: train_s, 1: val, 2: test}\n","    \n","    # Iterate over the keys (subdirectories) in the given directory\n","    for key in os.listdir(path_to_dir):\n","        source_path = os.path.join(path_to_dir, key)\n","        paths = os.listdir(source_path)\n","        \n","        # Define the destination paths for each set\n","        destination_path = [\n","            f\"/kaggle/working/train/{key}\",\n","            f\"/kaggle/working/validation/{key}\",\n","            f\"/kaggle/working/test/{key}\"\n","        ]\n","        \n","        # Create the destination directories if they don't exist\n","        for path_new in destination_path:\n","            if not os.path.exists(path_new):\n","                os.makedirs(path_new)\n","        \n","        i = 0\n","        # Iterate over the destination paths and copy the selected files\n","        for path_new in destination_path:\n","            selected_files_ = random.sample(paths, int(len(paths) * koef[i]))\n","            \n","            for file_name in selected_files_:\n","                destination_file = os.path.join(path_new, file_name)\n","                source_file = os.path.join(source_path, file_name)\n","                shutil.copy(source_file, destination_file)\n","            \n","            i += 1\n","    \n","    # Set the paths for the train, validation, and test sets\n","    train_path = '/kaggle/working/train/'\n","    val_path = '/kaggle/working/validation/'\n","    test_path = '/kaggle/working/test/'\n","    \n","    return train_path, val_path, test_path"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T02:53:36.103098Z","iopub.status.busy":"2024-02-24T02:53:36.102255Z","iopub.status.idle":"2024-02-24T02:53:40.989404Z","shell.execute_reply":"2024-02-24T02:53:40.988553Z","shell.execute_reply.started":"2024-02-24T02:53:36.103063Z"},"trusted":true},"outputs":[],"source":["train_path, val_path, test_path = gen_the_local_dataset(data_path)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T02:53:40.992021Z","iopub.status.busy":"2024-02-24T02:53:40.991237Z","iopub.status.idle":"2024-02-24T02:53:41.000057Z","shell.execute_reply":"2024-02-24T02:53:40.999106Z","shell.execute_reply.started":"2024-02-24T02:53:40.991983Z"},"trusted":true},"outputs":[],"source":["\n","def return_loader(train_path, val_path, test_path, video_transform, batch_size=4, numworker=3, pin_memory=True):\n","    \"\"\"\n","    Returns data loaders for training, validation, and testing datasets.\n","    \n","    Args:\n","        train_path (str): Path to the training data.\n","        val_path (str): Path to the validation data.\n","        test_path (str): Path to the testing data.\n","        video_transform (torchvision.transforms.Compose): Transformations to be applied to the video frames.\n","        batch_size (int, optional): Number of samples per batch. Defaults to 4.\n","        numworker (int, optional): Number of worker threads for data loading. Defaults to 3.\n","        pin_memory (bool, optional): If True, the data loader will copy tensors into pinned memory. Defaults to True.\n","    \n","    Returns:\n","        tuple: A tuple containing the training, validation, and testing data loaders.\n","    \"\"\"\n","    \n","    # Create training dataset\n","    train_dataset = pytorchvideo.data.Kinetics(\n","        data_path=train_path,\n","        clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", 2),\n","        transform=video_transform, \n","        decode_audio=False\n","    )\n","    \n","    # Create training data loader\n","    train_loader = DataLoader(\n","        train_dataset, batch_size,\n","        num_workers=numworker,\n","        pin_memory=pin_memory\n","    )\n","    \n","    #---------------------------------------------------------------------------------------------------------------------------------\n","    \n","    # Create validation dataset\n","    val_dataset = pytorchvideo.data.Kinetics(\n","        data_path=val_path,\n","        clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", 2),\n","        transform=video_transform, \n","        decode_audio=False\n","    )\n","    \n","    # Create validation data loader\n","    val_loader = DataLoader(\n","        val_dataset, batch_size,\n","        num_workers=numworker,\n","        pin_memory=pin_memory\n","    )\n","    \n","    #------------------------------------------------------------------------------------------------------------------------------------\n","    \n","    # Create testing dataset\n","    test_dataset = pytorchvideo.data.Kinetics(\n","        data_path=test_path,\n","        clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", 2),\n","        transform=video_transform, \n","        decode_audio=False\n","    )\n","    \n","    # Create testing data loader\n","    test_loader = DataLoader(\n","        test_dataset, batch_size,\n","        num_workers=numworker,\n","        pin_memory=pin_memory\n","    )\n","    \n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T02:53:41.001380Z","iopub.status.busy":"2024-02-24T02:53:41.001145Z","iopub.status.idle":"2024-02-24T02:53:41.033062Z","shell.execute_reply":"2024-02-24T02:53:41.032395Z","shell.execute_reply.started":"2024-02-24T02:53:41.001359Z"},"trusted":true},"outputs":[],"source":["train_loader, val_loader, test_loader = return_loader(train_path, val_path, test_path, video_transform, batch_size = 4, numworker = 3, pin_memory = True)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T03:00:27.603837Z","iopub.status.busy":"2024-02-24T03:00:27.603036Z","iopub.status.idle":"2024-02-24T03:00:27.620247Z","shell.execute_reply":"2024-02-24T03:00:27.619285Z","shell.execute_reply.started":"2024-02-24T03:00:27.603799Z"},"trusted":true},"outputs":[],"source":["class Model(LightningModule):\n","    \n","    def __init__(self):\n","        super(Model, self).__init__()\n","        \n","        # Load the pre-trained video model\n","        self.video_model = torch.hub.load('facebookresearch/pytorchvideo','efficient_x3d_xs', pretrained = True)\n","        \n","        # Activation function\n","        self.relu = nn.ReLU()\n","        \n","        # Linear layer for classification\n","        self.Linear = nn.Linear(400, 1)\n","\n","        # Learning rate\n","        self.lr = 1e-3\n","\n","        # Metrics for evaluation\n","        self.metrics = torchmetrics.Accuracy(task='binary')\n","        \n","        # Loss function\n","        self.criterion = nn.BCEWithLogitsLoss()\n","        \n","    def forward(self, x):\n","        x = self.relu(self.video_model(x))\n","        x = self.Linear(x)\n","        return x\n","    \n","    def configure_optimizers(self):\n","        # Configure the optimizer and learning rate scheduler\n","        optimizer = torch.optim.AdamW(params = self.parameters(), lr = self.lr)\n","        scheduler = lr_scheduler.StepLR(optimizer, step_size = 2, gamma = 0.1)\n","        \n","        return {\n","            'optimizer': optimizer,\n","            'lr_scheduler': scheduler\n","        }\n","        \n","    def _common_step(self, batch, batch_idx):\n","        video, label = batch['video'], batch['label']\n","        label = label.unsqueeze(1)\n","        scores = self.forward(video)\n","        \n","        loss = self.criterion(scores, label.to(torch.float32))\n","        metric = self.metrics(scores, label.to(torch.int64))\n","        \n","        return loss, scores, label\n","    \n","    def training_step(self, batch, batch_idx):\n","        loss, scores, labels = self._common_step(batch, batch_idx)\n","        metric = self.metrics(scores, labels.to(torch.int64))\n","        \n","        # Log training loss and accuracy\n","        self.log_dict({'train_loss': loss, 'training_accuracy':metric},\n","                       on_step = False, on_epoch = True, prog_bar = True)\n","        \n","        return {'loss': loss, 'metric': metric.detach()}\n","        \n","    def validation_step(self, batch, batch_idx):\n","        loss, scores, labels = self._common_step(batch, batch_idx)\n","        metric = self.metrics(scores, labels.to(torch.int64))\n","        \n","        # Log validation loss and accuracy\n","        self.log_dict({'val_loss': loss, 'val_accuracy':metric},\n","                       on_step = False, on_epoch = True, prog_bar = True)\n","        \n","        return {'loss': loss, 'metric': metric.detach()}\n","    \n","    def test_step(self, batch, batch_idx):\n","        loss, scores, labels = self._common_step(batch, batch_idx)\n","        metric = self.metrics(scores, labels.to(torch.int64))\n","        \n","        # Log test loss and accuracy\n","        self.log_dict({'test_loss': loss, 'test_accuracy':metric},\n","                       on_step = False, on_epoch = True, prog_bar = True)\n","        \n","        return {'loss': loss, 'metric': metric.detach()}\n","    \n","    def predict_step(self, batch, batch_idx):\n","        video, label = batch['video'], batch['batch']\n","        label = label.unsqueeze(1)\n","        scores = self.forward(video)\n","        preds = torch.argmax(scores, dim = 1)\n","        \n","        return preds\n","\n","    "]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T03:00:28.670952Z","iopub.status.busy":"2024-02-24T03:00:28.670249Z","iopub.status.idle":"2024-02-24T03:00:28.675728Z","shell.execute_reply":"2024-02-24T03:00:28.674852Z","shell.execute_reply.started":"2024-02-24T03:00:28.670911Z"},"trusted":true},"outputs":[],"source":["checkpoint_callback = ModelCheckpoint(monitor = 'val_loss', dirpath = 'checkpoints',\n","                                     filename = 'file', save_last = True)\n","\n","lr_monitor = LearningRateMonitor(logging_interval = 'epoch')"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T03:00:29.107020Z","iopub.status.busy":"2024-02-24T03:00:29.106692Z","iopub.status.idle":"2024-02-24T04:01:41.251641Z","shell.execute_reply":"2024-02-24T04:01:41.250627Z","shell.execute_reply.started":"2024-02-24T03:00:29.106982Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d84f9212e0e492fbff65cb70dfaaa45","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 8min 39s, sys: 51.9 s, total: 9min 31s\n","Wall time: 1h 1min 12s\n"]}],"source":["%%time\n","model = Model()\n","\n","trainer = Trainer(accelerator = 'gpu',\n","                 devices = [0],\n","                 min_epochs = 1,\n","                 max_epochs = 4,\n","                 callbacks = [lr_monitor, checkpoint_callback])\n","\n","trainer.fit(model, train_dataloaders = train_loader, val_dataloaders = val_loader)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T04:43:26.693514Z","iopub.status.busy":"2024-02-24T04:43:26.693105Z","iopub.status.idle":"2024-02-24T04:48:28.310483Z","shell.execute_reply":"2024-02-24T04:48:28.309568Z","shell.execute_reply.started":"2024-02-24T04:43:26.693476Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cb3671fb5a54c62a18e7a964e64d25e","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">       val_accuracy        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9796748161315918     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.061311185359954834    </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      val_accuracy       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9796748161315918    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.061311185359954834   \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'val_loss': 0.061311185359954834, 'val_accuracy': 0.9796748161315918}]"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["trainer.validate(model, val_loader )"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T04:48:28.313482Z","iopub.status.busy":"2024-02-24T04:48:28.312665Z","iopub.status.idle":"2024-02-24T04:51:13.181768Z","shell.execute_reply":"2024-02-24T04:51:13.180739Z","shell.execute_reply.started":"2024-02-24T04:48:28.313439Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53201fce83054d5bb43acca03e9fff35","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9836065769195557     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.059371467679739     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9836065769195557    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.059371467679739    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'test_loss': 0.059371467679739, 'test_accuracy': 0.9836065769195557}]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["trainer.test(model, test_loader )"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T05:24:11.292052Z","iopub.status.busy":"2024-02-24T05:24:11.291637Z","iopub.status.idle":"2024-02-24T05:24:12.012403Z","shell.execute_reply":"2024-02-24T05:24:12.011250Z","shell.execute_reply.started":"2024-02-24T05:24:11.292022Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"]}],"source":["\n","def save_model(model, filepath):\n","    \"\"\"\n","    Save the state dictionary of the model to the specified filepath.\n","    \n","    Args:\n","        model (nn.Module): The model to save.\n","        filepath (str): The filepath to save the model state dictionary.\n","    \"\"\"\n","    torch.save(model.state_dict(), filepath)\n","\n","#============================================================\n","\n","class Model_test(nn.Module):\n","    \"\"\"\n","    A custom model for testing real-life violence detection.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Model_test, self).__init__()\n","        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=False)\n","        self.relu = nn.ReLU()\n","        self.Linear = nn.Linear(400, 1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the model.\n","        \n","        Args:\n","            x (torch.Tensor): Input tensor.\n","        \n","        Returns:\n","            torch.Tensor: Output tensor.\n","        \"\"\"\n","        x = self.relu(self.video_model(x))\n","        x = self.Linear(x)\n","        return x\n","\n","#============================================================\n","\n","model_test = Model_test()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T05:24:47.488245Z","iopub.status.busy":"2024-02-24T05:24:47.487334Z","iopub.status.idle":"2024-02-24T05:24:47.560297Z","shell.execute_reply":"2024-02-24T05:24:47.559343Z","shell.execute_reply.started":"2024-02-24T05:24:47.488207Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model_test.load_state_dict(torch.load('/kaggle/working/model.pth'))"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T05:25:23.332586Z","iopub.status.busy":"2024-02-24T05:25:23.331735Z","iopub.status.idle":"2024-02-24T05:25:23.408830Z","shell.execute_reply":"2024-02-24T05:25:23.407897Z","shell.execute_reply.started":"2024-02-24T05:25:23.332548Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([3, 20, 224, 224])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["from pytorchvideo.data.encoded_video import EncodedVideo\n","\n","video = EncodedVideo.from_path('/kaggle/working/test/NonViolence/NV_424.mp4')\n","\n","video_data = video.get_clip(0,2)\n","\n","video_data = video_transform(video_data)\n","\n","video_data['video'].shape"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T05:25:24.978940Z","iopub.status.busy":"2024-02-24T05:25:24.978567Z","iopub.status.idle":"2024-02-24T05:25:25.017889Z","shell.execute_reply":"2024-02-24T05:25:25.016943Z","shell.execute_reply.started":"2024-02-24T05:25:24.978910Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 3, 20, 224, 224])"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["model = model.cuda()\n","\n","inputs = video_data['video'].cuda()\n","\n","inputs = torch.unsqueeze(inputs, 0 )\n","inputs.shape"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T05:25:25.980327Z","iopub.status.busy":"2024-02-24T05:25:25.979944Z","iopub.status.idle":"2024-02-24T05:25:26.051688Z","shell.execute_reply":"2024-02-24T05:25:26.050574Z","shell.execute_reply.started":"2024-02-24T05:25:25.980296Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[-7.669764]], dtype=float32)"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","preds = model(inputs)\n","\n","preds = preds.detach().cpu().numpy()\n","\n","preds\n","\n"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T05:25:27.965988Z","iopub.status.busy":"2024-02-24T05:25:27.965259Z","iopub.status.idle":"2024-02-24T05:25:27.970646Z","shell.execute_reply":"2024-02-24T05:25:27.969426Z","shell.execute_reply.started":"2024-02-24T05:25:27.965954Z"},"trusted":true},"outputs":[],"source":["preds = np.where(preds>0.5,1,0)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T05:25:29.736183Z","iopub.status.busy":"2024-02-24T05:25:29.735556Z","iopub.status.idle":"2024-02-24T05:25:29.741939Z","shell.execute_reply":"2024-02-24T05:25:29.741045Z","shell.execute_reply.started":"2024-02-24T05:25:29.736151Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[0]])"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":176381,"sourceId":397693,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
